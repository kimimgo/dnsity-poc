"""
NoLiMa ë…¼ë¬¸ ê¸°ì¤€ìœ¼ë¡œ í˜„ì¬ NIAH ë°ì´í„°ì…‹ì˜ ë¬¸ì œì ì„ ì‹¬ì¸µ ë¶„ì„
"""
import json
import re
from collections import Counter
import numpy as np

def analyze_question_diversity(samples):
    """ì§ˆë¬¸ ë‹¤ì–‘ì„± ë¶„ì„"""
    questions = [s['question'] for s in samples]
    unique_questions = set(questions)

    return {
        'total_samples': len(samples),
        'unique_questions': len(unique_questions),
        'diversity_ratio': len(unique_questions) / len(samples),
        'most_common': Counter(questions).most_common(5)
    }

def analyze_needle_pattern(samples):
    """Needle íŒ¨í„´ ë¶„ì„"""
    patterns = []
    for sample in samples:
        context = sample['context']
        answer = sample['answer']

        # Needle ì°¾ê¸°
        needle_pattern = rf"[Tt]he secret passkey is {answer}"
        match = re.search(needle_pattern, context, re.IGNORECASE)

        if match:
            # Needle ì „í›„ 50ì ì¶”ì¶œ
            start = max(0, match.start() - 50)
            end = min(len(context), match.end() + 50)
            patterns.append(context[start:end])

    return patterns[:10]  # ì²˜ìŒ 10ê°œë§Œ

def analyze_context_repetition(sample):
    """ë¬¸ë§¥ ë‚´ ë¬¸ì¥ ë°˜ë³µë„ ë¶„ì„"""
    context = sample['context']
    sentences = [s.strip() for s in re.split(r'[.!?]+', context) if s.strip()]

    # ê° ë¬¸ì¥ì˜ ë“±ì¥ íšŸìˆ˜
    sentence_counts = Counter(sentences)
    repeated_sentences = {s: c for s, c in sentence_counts.items() if c > 1}

    # ê°€ì¥ ë§ì´ ë°˜ë³µëœ ë¬¸ì¥ë“¤
    most_repeated = sentence_counts.most_common(5)

    total_sentences = len(sentences)
    unique_sentences = len(set(sentences))

    return {
        'total_sentences': total_sentences,
        'unique_sentences': unique_sentences,
        'repetition_ratio': (total_sentences - unique_sentences) / total_sentences if total_sentences > 0 else 0,
        'most_repeated': most_repeated,
        'num_repeated_types': len(repeated_sentences)
    }

def simulate_keyword_matching(sample):
    """í‚¤ì›Œë“œ ë§¤ì¹­ë§Œìœ¼ë¡œ ì •ë‹µì„ ì°¾ì„ ìˆ˜ ìˆëŠ”ì§€ ì‹œë®¬ë ˆì´ì…˜"""
    question = sample['question'].lower()
    context = sample['context'].lower()
    answer = sample['answer'].lower()

    # ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = set(re.findall(r'\b\w+\b', question)) - {'the', 'is', 'in', 'what', 'a', 'an', 'to', 'of', 'and'}

    # "secret passkey" ì£¼ë³€ ì°¾ê¸°
    if 'secret' in keywords and 'passkey' in keywords:
        # "secret passkey is" íŒ¨í„´ ì°¾ê¸°
        pattern = r'secret\s+passkey\s+is\s+([A-Z0-9]{6})'
        match = re.search(pattern, context, re.IGNORECASE)

        if match:
            predicted = match.group(1).lower()
            return predicted == answer

    return False

def calculate_nolima_score(samples):
    """NoLiMa ê¸°ì¤€ ì ìˆ˜ ê³„ì‚° (0-100)"""
    scores = {
        'question_diversity': 0,  # ì§ˆë¬¸ ë‹¤ì–‘ì„± (30ì )
        'lexical_decoupling': 0,  # ì–´íœ˜ ë¶„ë¦¬ë„ (30ì )
        'context_complexity': 0,  # ë¬¸ë§¥ ë³µì¡ë„ (20ì )
        'pattern_generalization': 0  # íŒ¨í„´ ì¼ë°˜í™” (20ì )
    }

    # 1. ì§ˆë¬¸ ë‹¤ì–‘ì„±
    q_analysis = analyze_question_diversity(samples)
    scores['question_diversity'] = q_analysis['diversity_ratio'] * 30

    # 2. ì–´íœ˜ ë¶„ë¦¬ë„ (í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ í•´ê²° ë¶ˆê°€)
    keyword_solvable = sum(1 for s in samples[:50] if simulate_keyword_matching(s))
    scores['lexical_decoupling'] = (1 - keyword_solvable/50) * 30

    # 3. ë¬¸ë§¥ ë³µì¡ë„ (ë°˜ë³µì´ ì ì„ìˆ˜ë¡ ë†’ìŒ)
    rep_ratios = [analyze_context_repetition(s)['repetition_ratio'] for s in samples[:20]]
    avg_rep = np.mean(rep_ratios)
    scores['context_complexity'] = (1 - avg_rep) * 20

    # 4. íŒ¨í„´ ì¼ë°˜í™” (Needle íŒ¨í„´ì´ ë‹¤ì–‘í• ìˆ˜ë¡ ë†’ìŒ)
    patterns = analyze_needle_pattern(samples[:20])
    unique_patterns = len(set(patterns))
    scores['pattern_generalization'] = (unique_patterns / len(patterns)) * 20 if patterns else 0

    return scores

def main():
    # Load datasets
    with open('data/processed/niah/global_niah.jsonl') as f:
        global_data = [json.loads(line) for line in f]

    with open('data/processed/niah/korean_niah.jsonl') as f:
        korean_data = [json.loads(line) for line in f]

    for name, data in [("Global", global_data), ("Korean", korean_data)]:
        print(f"\n{'='*80}")
        print(f"ğŸ“‹ {name} NIAH - NoLiMa ì¤€ìˆ˜ë„ ë¶„ì„")
        print(f"{'='*80}")

        # 1. ì§ˆë¬¸ ë‹¤ì–‘ì„±
        print(f"\nğŸ¯ 1. ì§ˆë¬¸ ë‹¤ì–‘ì„±")
        q_div = analyze_question_diversity(data)
        print(f"  ì´ ìƒ˜í”Œ: {q_div['total_samples']}ê°œ")
        print(f"  ê³ ìœ  ì§ˆë¬¸: {q_div['unique_questions']}ê°œ")
        print(f"  ë‹¤ì–‘ì„± ë¹„ìœ¨: {q_div['diversity_ratio']*100:.1f}%")
        print(f"  ê°€ì¥ ë¹ˆë²ˆí•œ ì§ˆë¬¸ (ìƒìœ„ 3ê°œ):")
        for q, count in q_div['most_common'][:3]:
            print(f"    \"{q[:60]}...\" - {count}íšŒ")

        # 2. Needle íŒ¨í„´
        print(f"\nğŸ” 2. Needle íŒ¨í„´ ë¶„ì„")
        patterns = analyze_needle_pattern(data)
        print(f"  ìƒ˜í”Œ íŒ¨í„´ (ì²˜ìŒ 3ê°œ):")
        for i, p in enumerate(patterns[:3]):
            print(f"    #{i+1}: ...{p}...")

        # 3. ë¬¸ë§¥ ë°˜ë³µë„
        print(f"\nğŸ“Š 3. ë¬¸ë§¥ ë°˜ë³µë„ ë¶„ì„ (ìƒ˜í”Œ 1ê°œ)")
        rep_analysis = analyze_context_repetition(data[0])
        print(f"  ì´ ë¬¸ì¥ ìˆ˜: {rep_analysis['total_sentences']}ê°œ")
        print(f"  ê³ ìœ  ë¬¸ì¥ ìˆ˜: {rep_analysis['unique_sentences']}ê°œ")
        print(f"  ë°˜ë³µ ë¹„ìœ¨: {rep_analysis['repetition_ratio']*100:.1f}%")
        print(f"  ê°€ì¥ ë§ì´ ë°˜ë³µëœ ë¬¸ì¥ (ìƒìœ„ 3ê°œ):")
        for sent, count in rep_analysis['most_repeated'][:3]:
            print(f"    {count}íšŒ: \"{sent[:70]}...\"")

        # 4. í‚¤ì›Œë“œ ë§¤ì¹­ ì‹œë®¬ë ˆì´ì…˜
        print(f"\nğŸ¤– 4. í‚¤ì›Œë“œ ë§¤ì¹­ ì‹œë®¬ë ˆì´ì…˜ (ì²˜ìŒ 50ê°œ)")
        keyword_solved = sum(1 for s in data[:50] if simulate_keyword_matching(s))
        print(f"  í‚¤ì›Œë“œ ë§¤ì¹­ë§Œìœ¼ë¡œ í•´ê²° ê°€ëŠ¥: {keyword_solved}/50 ({keyword_solved/50*100:.0f}%)")

        # 5. NoLiMa ì ìˆ˜
        print(f"\nğŸ“ 5. NoLiMa ì¤€ìˆ˜ë„ ì ìˆ˜")
        scores = calculate_nolima_score(data)
        total_score = sum(scores.values())
        print(f"  ì§ˆë¬¸ ë‹¤ì–‘ì„± (30ì  ë§Œì ): {scores['question_diversity']:.1f}ì ")
        print(f"  ì–´íœ˜ ë¶„ë¦¬ë„ (30ì  ë§Œì ): {scores['lexical_decoupling']:.1f}ì ")
        print(f"  ë¬¸ë§¥ ë³µì¡ë„ (20ì  ë§Œì ): {scores['context_complexity']:.1f}ì ")
        print(f"  íŒ¨í„´ ì¼ë°˜í™” (20ì  ë§Œì ): {scores['pattern_generalization']:.1f}ì ")
        print(f"  \n  ğŸ“ˆ ì´ì : {total_score:.1f}/100ì ")

        # 6. NoLiMa ê°œì„  ê¶Œì¥ì‚¬í•­
        print(f"\nğŸ’¡ 6. NoLiMa ê¸°ì¤€ ê°œì„  ê¶Œì¥ì‚¬í•­")
        recommendations = []

        if scores['question_diversity'] < 15:
            recommendations.append("âŒ ì§ˆë¬¸ì„ ë‹¤ì–‘í™”í•˜ì„¸ìš” (í˜„ì¬ ê±°ì˜ ë™ì¼)")

        if scores['lexical_decoupling'] < 15:
            recommendations.append("âŒ ì§ˆë¬¸ê³¼ needleì˜ ì–´íœ˜ ì¤‘ë³µì„ ì¤„ì´ì„¸ìš”")

        if scores['context_complexity'] < 10:
            recommendations.append("âŒ ë¬¸ë§¥ì˜ ë°˜ë³µì„ ì¤„ì´ê³  ë‹¤ì–‘ì„±ì„ ë†’ì´ì„¸ìš”")

        if scores['pattern_generalization'] < 10:
            recommendations.append("âŒ Needle í‘œí˜„ ë°©ì‹ì„ ë‹¤ì–‘í™”í•˜ì„¸ìš”")

        if recommendations:
            for rec in recommendations:
                print(f"  {rec}")
        else:
            print(f"  âœ… ì£¼ìš” ê°œì„ ì‚¬í•­ ì—†ìŒ")

if __name__ == "__main__":
    main()
