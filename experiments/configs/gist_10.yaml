model:
  name: meta-llama/Meta-Llama-3-8B-Instruct
  num_gist_tokens: 10
  load_in_4bit: true

lora:
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
  modules_to_save:
    - embed_tokens
    - lm_head

training:
  learning_rate: 0.0001
  warmup_steps: 100
  max_steps: 1000
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 8
