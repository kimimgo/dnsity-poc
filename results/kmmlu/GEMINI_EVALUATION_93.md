# KMMLU 실험 Gemini 평가 결과

**평가일**: 2026-01-06
**평가 모델**: Gemini 3 Pro Preview
**이전 평가**: 70/100 (30 샘플, Gist 평가 누락)
**최종 점수**: 93/100 (+23점 개선)

## 항목별 점수

| 항목 | 점수 | 평가 |
|------|------|------|
| **실험 설계** | 27/30 | 샘플 확장 및 대조군 완비로 신뢰성 확보 |
| **기술 완성도** | 23/25 | 치명적 기술 이슈(OOM) 해결 및 파이프라인 완주 |
| **결과 분석** | 24/25 | 태스크 성격(추출 vs 추론)에 따른 모델 특성 규명 |
| **실용 가치** | 19/20 | 명확한 도입 가이드라인 제시 |
| **총점** | **93/100** | **Excellent** |

## 주요 평가 내용

### 강점

1. **샘플 확장**: 30개 → 100개로 3배 이상 확장하여 통계적 신뢰도 향상
2. **OOM 해결**: 8-bit 양자화 + CPU Offload로 12GB adapter 로드 성공
3. **핵심 인사이트 도출**: "Gist는 정보 추출(NIAH)엔 강하나 복합 추론(KMMLU)엔 약하다"
4. **방법론별 적합 용도 테이블**: 비즈니스 의사결정에 즉시 활용 가능한 가이드

### 개선 필요 사항

1. 전체 데이터셋(1,000개) 대비 10% 수준
2. LoRA 하이퍼파라미터 튜닝 여지
3. Gist 구현이 '단순 주입' 방식
4. RAG 검색 정확도(Retriever 자체 성능) 분석 보완

## 총평

> 비록 LoRA와 Gist가 Baseline보다 낮은 성능을 보였지만, 이는 실패한 실험이 아닙니다. **"소량 데이터 환경에서의 전문 지식 문제 해결에는 RAG가 가장 효과적이며, Gist나 LoRA는 각각 문맥 압축과 대량 학습이라는 다른 목적에 적합하다"**는 귀중한 **Negative Result**를 얻었기 때문입니다.

## 실험 결과 요약

| 방법 | 정확도 | vs Baseline |
|------|--------|-------------|
| Baseline (Zero-shot) | 25.00% | - |
| RAG (BGE-M3, Top-3) | 31.00% | **+6.00%p** |
| LoRA (2 epochs) | 24.00% | -1.00%p |
| Gist Token (25 tokens) | 23.00% | -2.00%p |

**결론**: 전문 지식 Q&A에는 RAG가 유일한 효과적 방법
